package uk.gov.moj.cp.retrieval.service;

import static uk.gov.moj.cp.ai.util.StringUtil.isNullOrEmpty;

import uk.gov.moj.cp.ai.model.ChunkedEntry;
import uk.gov.moj.cp.ai.model.KeyValuePair;
import uk.gov.moj.cp.ai.service.ChatService;

import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.Set;
import java.util.stream.Collectors;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class ResponseGenerationService {

    private static final Logger LOGGER = LoggerFactory.getLogger(ResponseGenerationService.class);

    private static final String SYSTEM_PROMPT_TEMPLATE = """
            You are an expert Legal Advisor. You provide exhaustive, detail-oriented responses based strictly on the provided documents.
            
            **Retrieved Documents:**
            %s
            
            **Instructions:**
            
            1. Strict Document Adherence: Answer the user's query only using information found within the {Retrieved Documents}. If the information is not present, state that it is unavailable in the provided records.
            
            2. The "Single Placeholder" Mandate: * Every factual statement or sentence must be followed by exactly one sequential numerical placeholder (e.g., [1]).
                - Placeholder must always be enclosed in square brackets. (e.g, [1])
                - PROHIBITION: You are strictly forbidden from placing placeholders back-to-back (e.g., [1][2] or [1, 2] or [1],[2]).
                - If a statement draws from multiple pages, you MUST aggregate all those sources into the single corresponding JSON object for that citation ID.
            
            3. Source Aggregation and mandatory incremental citation logic:
                - If Statement A is supported by Page 5 of Doc 1 and Page 10 of Doc 1: Use [1]. In the JSON, pageNumbers becomes "5, 10".
                - Sentence-Level Citations: Every individual fact or bullet point MUST have a citation.
                - Incrementing IDs: You must use sequential numbering. The first distinct fact/sentence gets [1], the second gets [2], the third gets [3], and so on.
                - Prohibition of "The Global Citation": Do not wait until the end of a paragraph to use one single citation for everything. Each distinct factual claim needs its own incremental marker.
            
            4. JSON Page Formatting:
                - individualPageNumbers: List all cited page numbers, comma-separated (e.g., "17,18,19,20,21").
                - pageNumbers: Compress consecutive page numbers using a hyphen (e.g., "17-21,24").
            
            5. Formatting & Guardrails:
                - Use (1), (2), (i), (ii) or bullet points for lists. Never use square brackets [] for anything other than citations.
                - Heading Hierarchy: Follow DAC/NFT accessibility standards. Use ## for main titles, ### for subheadings, and #### for further nesting. Never use #.
            
            6. Data Output (FACT_MAP_JSON):
                - Immediately following the response text, generate a single, minified JSON array.
                - Wrap the array in literal tags: <FACT_MAP_JSON>[{"citationId": 1, ...}]</FACT_MAP_JSON>.
                - JSON Schema: Each object must contain: "citationId", "documentFilename", "pageNumbers", "individualPageNumbers", and "documentId".
            
            Additional Context: %s
            
            Provide the answer in a professional, formal legal tone. Do not ask a follow-up query at the end.
            """;

    private final ChatService chatService;
    private final CitationProcessor citationProcessor;

    public ResponseGenerationService() {
        String endpoint = System.getenv("AZURE_OPENAI_ENDPOINT");
        String deploymentName = System.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT_NAME");

        // Using managed identity - pass null for API key to enable managed identity
        chatService = new ChatService(endpoint, deploymentName);
        citationProcessor = new CitationProcessor();
    }

    public ResponseGenerationService(final ChatService chatService, final CitationProcessor citationProcessor) {
        this.chatService = chatService;
        this.citationProcessor = citationProcessor;
    }

    public String generateResponse(final String userQuery, final List<ChunkedEntry> chunkedEntries, final String userQueryPrompt) {
        LOGGER.info("Generating LLM response for query: {}", userQuery);

        if(null == chunkedEntries || chunkedEntries.isEmpty()) {
            LOGGER.warn("No matching data from search database retrieved for query: {}", userQuery);
            return "No response generated by the service.";
        }

        String retrievedContextsString = buildContextString(chunkedEntries);
        String systemPromptContent = String.format(SYSTEM_PROMPT_TEMPLATE, retrievedContextsString,
                userQueryPrompt != null ? userQueryPrompt : "");

        try {
            return chatService.callModel(systemPromptContent, userQuery, String.class)
                    .filter(response -> !isNullOrEmpty(response))
                    .map(response -> {
                        String trimmedResponse = citationProcessor.processAndFormatCitations(response);
                        LOGGER.info("LLM Raw Response length = {}", trimmedResponse.length());
                        return trimmedResponse;
                    })
                    .orElseGet(() -> {
                        LOGGER.warn("LLM returned no response.");
                        return "No response generated by the service.";
                    });
        } catch (Exception e) {
            LOGGER.error("Error generating response for user query: {}", userQuery, e);
            return "An error occurred while generating the response.";
        }
    }

    private String buildContextString(final List<ChunkedEntry> chunkedEntries) {
        if (chunkedEntries == null || chunkedEntries.isEmpty()) {
            return "No relevant documents were retrieved for this query";
        }

        Map<String, List<ChunkedEntry>> groupedEntries = chunkedEntries.stream()
                .collect(Collectors.groupingBy(ChunkedEntry::documentId));

        StringBuilder contextBuilder = new StringBuilder();

        for (Map.Entry<String, List<ChunkedEntry>> group : groupedEntries.entrySet()) {
            List<ChunkedEntry> docs = group.getValue();
            final String documentId = group.getKey();
            final String fileName = extractMaterialName(docs.get(0)).orElse(docs.get(0).documentFileName());

            // Header for the Document
            contextBuilder.append("--- START OF DOCUMENT ---\n");
            contextBuilder.append("DOCUMENT_ID: ").append(documentId).append("\n");
            contextBuilder.append("DOCUMENT_FILENAME: ").append(fileName).append("\n");

            // Append all chunks for this document
            for (ChunkedEntry entry : docs) {
                if (entry.pageNumber() != null) {
                    contextBuilder.append("[PAGE: ").append(entry.pageNumber()).append("] ");
                }
                contextBuilder.append(entry.chunk()).append("\n");
            }
            contextBuilder.append("--- END OF DOCUMENT ---\n\n");
        }
        return contextBuilder.toString();
    }

    private Optional<String> extractMaterialName(ChunkedEntry entry) {
        if (entry.customMetadata() == null || entry.customMetadata().isEmpty()) {
            return Optional.empty();
        }

        return entry.customMetadata().stream()
                .filter(pair -> "material_name".equals(pair.key()))
                .map(KeyValuePair::value)
                .filter(value -> !isNullOrEmpty(value))
                .findFirst();
    }
}